{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xtiPZf8B-I9"
   },
   "source": [
    "# Seminar 1. Likelihood-based models.\n",
    "\n",
    "This seminar will be about likelihood-based models: autoregressive and flow-based. Agenda:\n",
    "- Likelihood model in 1D - fitting histogram using SGD (2 points)\n",
    "- Deep Autoregressive model via Transformer on Shapes and Binarized MNIST (5 points)\n",
    "- Conditional Autoregressive model via Transformer (3 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPhLCmFRCr6J"
   },
   "source": [
    "# Part 1. Fitting histogram.\n",
    "\n",
    "In this part we will build our first likelihood-based model for 1D data and will try to fit it using gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9y32D_wcB-uv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b951weRPC8tQ"
   },
   "source": [
    "Choose your device: don't forget to switch to GPU runtime when working in collab with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjX1Cj8sC4ra"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aVD15MiDH9l"
   },
   "source": [
    "First, we define the procedure of data generation. It will generate a dataset of samples $x \\in \\{0 \\dots 99\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LABHN0l9DA3T"
   },
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    count = 10000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = 0.3 + 0.1 * rand.randn(count)\n",
    "    b = 0.8 + 0.05 * rand.randn(count)\n",
    "    mask = rand.rand(count) < 0.5\n",
    "    samples = np.clip(a * mask + b * (1 - mask), 0.0, 1.0)\n",
    "    \n",
    "    return np.digitize(samples, np.linspace(0.0, 1.0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT3Ji5sFDOio"
   },
   "source": [
    "We generate data and perform train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDujw_WFDMeX"
   },
   "outputs": [],
   "source": [
    "data = sample_data()\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3)\n",
    "train_data, val_data = train_test_split(train_data, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O44NBI2XDRrg"
   },
   "source": [
    "Let's plot and visualize the histogram of training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPslSdd3DUZF"
   },
   "outputs": [],
   "source": [
    "def plot_histogram(data):\n",
    "    counts = Counter(data)\n",
    "    keys = list(counts.keys())\n",
    "    values = list(counts.values())\n",
    "    plt.bar(keys, values)\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NtWa6euDZw7"
   },
   "source": [
    "On lecture we have discussed how to build histogram model. But this model is not the best choice for high-dimensional data. So, we suggesst to you to implement the following parametrized model:\n",
    "\n",
    "$$ p_\\theta(x)_i = \\frac{e^{\\theta_i}}{\\sum_j{e^{\\theta_j}}} $$\n",
    "\n",
    "Where $\\theta=(\\theta_0 \\dots \\theta_{99})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ToYJWA3Dfio"
   },
   "source": [
    "We propose you to implement this model in the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X094KzyTDccu"
   },
   "outputs": [],
   "source": [
    "class SimpleProbabilityModel(nn.Module):\n",
    "    # Store all parameters of your model as class fields in constructor\n",
    "    def __init__(self,  num_elements=100):\n",
    "        super(SimpleProbabilityModel, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        \n",
    "    # Forward should return vector of log probabilities for each element\n",
    "    def forward(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Should sample element using probabilities, obtained from parameters. Return single number 0..99\n",
    "    def sample(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUpMKqeIDjD-"
   },
   "source": [
    "We will train this model using negative log-likelihood optimization: $ L_i = -\\log p_{y_i} $. Implement this loss calculation for your model given a batch of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_OXdVmZDjOq"
   },
   "outputs": [],
   "source": [
    "# data: n.array of numbers from your training distribution\n",
    "# model: instance of your SimpleProbabilityModel.\n",
    "# should return: negative log-likelihood of your data given the model to perform backpropagation\n",
    "def calc_loss(data, model):\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1ZyrWFFDyO0"
   },
   "source": [
    "Finally, we can create instance of our model and perform training. Note that if your calculated previous loss as classic natural logarithm, here we scale it to binary logarithm for logging likelihood in bits (which is better for interpretation and comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SZDdLG0Dy-A"
   },
   "outputs": [],
   "source": [
    "model = SimpleProbabilityModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrbTVMKwD0un"
   },
   "outputs": [],
   "source": [
    "def train_simple_model(model, train_data, val_data, num_epochs=20000, batch_size=4000, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(len(train_data) // batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch = train_data[batch_size * j:batch_size * (j + 1)]\n",
    "            l = calc_loss(batch, model)\n",
    "            train_losses.append(l.item() / math.log(2))\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        l = calc_loss(val_data, model)\n",
    "        val_losses.append(l.item() / math.log(2))\n",
    "    \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Val NLL(bits)\")\n",
    "    plt.plot(val_losses, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Final validation NLL(bits): {}\".format(val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoJzczt1D2rE"
   },
   "outputs": [],
   "source": [
    "train_simple_model(model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s50JgfFPD9cB"
   },
   "source": [
    "You can also tune your training parameters (number of epochs, batch size, learning rate, optimizer), to improve validation NLL. You should obtain something below 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sex9DB6FEBjt"
   },
   "source": [
    "Finally, let's sample values from our model and visualize histograms of our test data and our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0kiuE9YD5Ls"
   },
   "outputs": [],
   "source": [
    "sampled_data = [model.sample().cpu().item() for _ in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW983t-0EEDp"
   },
   "outputs": [],
   "source": [
    "plot_histogram(sampled_data)\n",
    "plot_histogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Transformer as universal autoregressive model\n",
    "\n",
    "In this part, implement a simple Transformer architecture to model binary MNIST and shapes images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_samples(samples, fname=None, nrow=10, title='Samples'):\n",
    "    samples = torch.FloatTensor(samples)\n",
    "    if len(samples.shape) == 3:\n",
    "        samples = samples.unsqueeze(-1)\n",
    "    samples = samples.permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def load_data(fname, include_labels=False):\n",
    "    with open(fname, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "\n",
    "    train_data = (data['train'] > 127.5).astype(np.int32)\n",
    "    test_data = (data['test'] > 127.5).astype(np.int32)\n",
    "    if include_labels:\n",
    "        return train_data, test_data, data['train_labels'], data['test_labels']\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/shapes.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_train, shapes_test, train_labels, test_labels = load_data('./shapes.pkl', True)\n",
    "show_samples(shapes_train[:100, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the following network design:\n",
    "\n",
    "- Trainable PositionalEmbeddings\n",
    "- N-layer Transformer Encoder (with causal mask)\n",
    "- (!) norm_first=True\n",
    "- logits as an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, num_tokens, max_len):\n",
    "        super().__init__()\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq_size, batch_size = x.shape\n",
    "        positions = torch.arange(0, seq_size, 1, dtype=torch.long, device=x.device)\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "    def loss(self, x: torch.Tensor):\n",
    "        # [seq_len, bs] -> [bs, seq_len]\n",
    "        target = x[1:].transpose(0, 1)\n",
    "        # [seq_len, bs, num_tokens] -> [bs, num_tokens, seq_len]\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "\n",
    "def sample(model: nn.Module, n_samples, start_token, out_len):\n",
    "    # [seq_size, batch_size]\n",
    "    output = torch.zeros(out_len + 1, n_samples, dtype=torch.long)\n",
    "    output[0] = start_token\n",
    "    output = output.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for t in range(out_len):\n",
    "        with torch.no_grad():\n",
    "            x = output\n",
    "            # [batch_size, num_tokens]\n",
    "            logits = model.forward(x)\n",
    "            probs = F.softmax(logits, dim=-1)[t]\n",
    "            next = torch.multinomial(probs, 1).squeeze(-1)\n",
    "            output[t + 1] = next\n",
    "    return output[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = shapes_train[0].shape\n",
    "shapes_model = TransformerModel(2, 128, 3, H * W + 1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sample(shapes_model, 8, 0, H * W).cpu().numpy()\n",
    "assert out.shape == (H * W, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconditional tokenizer\n",
    "\n",
    "Implement simple unconditional image tokenizer: first element should be BOS, then your flattened image\n",
    "\n",
    "- encoder accepts a single image and returns a sequence\n",
    "- decoder accepts a single sequence WITHOUT leading BOS and returns an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTokenizer:\n",
    "    def __init__(self, height, width):\n",
    "        self.bos = 2\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def encode(self, x: np.ndarray):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "    def decode(self, x: np.ndarray):\n",
    "        bos = (x == self.bos)\n",
    "        if bos.sum() > 0:\n",
    "            print(f\"warning: bad trained model, all bos will be replaced to zero token\")\n",
    "            x[bos] = 0\n",
    "        return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_tokenizer = ImageTokenizer(H, W)\n",
    "encoded = shapes_tokenizer.encode(shapes_train[10].squeeze(-1))\n",
    "decoded = shapes_tokenizer.decode(encoded[1:])\n",
    "assert np.allclose(shapes_train[10].squeeze(-1), decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = shapes_model.loss(torch.tensor(encoded, dtype=torch.long, device=device).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, X, _, tokenizer):\n",
    "        super().__init__()\n",
    "        self.X = X.squeeze(-1)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenizer.encode(self.X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x.to(device=device, dtype=torch.long)\n",
    "        loss = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.transpose(0, 1)\n",
    "            x = x.to(device=device, dtype=torch.long)\n",
    "            loss = model.loss(x)\n",
    "            total_loss += loss * x.shape[1]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_loss(model, test_loader)]\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch} started')\n",
    "        model.train()\n",
    "        train_losses.extend(train(model, train_loader, optimizer))\n",
    "        test_loss = eval_loss(model, test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        print('train loss: {}, test_loss: {}'.format(np.mean(train_losses[-1000:]), \n",
    "                                                     test_losses[-1]))\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_model(train_data, test_data, train_labels, test_labels, model, tokenizer, dataset_cls):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    model: nn.Model item, should contain function loss\n",
    "    tokenizer: ImageTokenizer or LabeledImageTokenizer instance\n",
    "    dataset_cls: dataset constructor, should accept data, labels and tokenizer as arguments\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = shapes_train[0].shape\n",
    "tokenizer = # TODO\n",
    "shapes_model = # TODO\n",
    "\n",
    "train_losses, test_losses, shapes_model = train_model(shapes_train, shapes_test, train_labels, test_labels,\n",
    "                                                      shapes_model, tokenizer, TokenizedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_plots(train_losses, test_losses, title):\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(shapes_model, 100, tokenizer.bos, H * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = np.zeros((100, H, W), dtype=np.int64)\n",
    "################\n",
    "# YOUR CODE HERE\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation\n",
    "\n",
    "Let's try to train our autoregressive model with simple conditioning: instead of BOS token we'll use class token at start of our sequence\n",
    "\n",
    "There are two things we need to change: our tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledImageTokenizer:\n",
    "    def __init__(self, height, width, num_tokens=2):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.first_label = num_tokens\n",
    "\n",
    "    def encode(self, x: np.ndarray, label: int):\n",
    "        x = x.flatten()\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        return out\n",
    "\n",
    "    def encode_label(self, label):\n",
    "        return # TODO\n",
    "\n",
    "    def decode(self, x: np.ndarray):\n",
    "        labels = (x > 1)\n",
    "        if labels.sum() > 0:\n",
    "            print(f\"warning: bad trained model, all labels will be replaced to zero token\")\n",
    "            x[labels] = 0\n",
    "        return x.reshape(self.height, self.width)\n",
    "\n",
    "\n",
    "class TokenizedDatasetWithLabel(Dataset):\n",
    "    def __init__(self, X, labels, tokenizer):\n",
    "        super().__init__()\n",
    "        self.X = X.squeeze(-1)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenizer.encode(self.X[index], self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = # TODO\n",
    "H, W, _ = shapes_train[0].shape\n",
    "shapes_model = # TODO\n",
    "\n",
    "train_losses, test_losses, shapes_model = train_model(shapes_train, shapes_test, train_labels, test_labels, \n",
    "                                                      shapes_model, tokenizer, TokenizedDatasetWithLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'Shapes-conditional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((100, H, W))\n",
    "n_samples = 100 // n_labels\n",
    "for label in range(n_labels):\n",
    "    first_token = tokenizer.encode_label(label)\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second dataset: MNIST\n",
    "\n",
    "Ensure that your model and code are working for more complex dataset too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/mnist.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test, train_labels, test_labels = load_data('./mnist.pkl', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(mnist_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = mnist_train[0].shape\n",
    "model = TransformerModel(2, 128, 3, H * W + 1).to(device=device)\n",
    "tokenizer = ImageTokenizer(H, W)\n",
    "train_losses, test_losses, model = train_model(mnist_train, mnist_test, train_labels, test_labels, \n",
    "                                               model, tokenizer, TokenizedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(model, 100, tokenizer.bos, H * W)\n",
    "decoded = np.zeros((100, H, W), dtype=np.int64)\n",
    "################\n",
    "# YOUR CODE HERE\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And conditional generation too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(set(train_labels))\n",
    "tokenizer = LabeledImageTokenizer(H, W)\n",
    "H, W, _ = mnist_train[0].shape\n",
    "model = TransformerModel(2, 128, 2 + n_labels, H * W + 1).to(device=device)\n",
    "\n",
    "train_losses, test_losses, model = train_model(mnist_train, mnist_test, train_labels, test_labels, \n",
    "                                               model, tokenizer, TokenizedDatasetWithLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'MNIST-conditional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((100, H, W))\n",
    "n_samples = 100 // n_labels\n",
    "for label in range(n_labels):\n",
    "    first_token = tokenizer.encode_label(label)\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(samples)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
