{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6b2f07",
   "metadata": {},
   "source": [
    "# Homework 1. Likelihood-based models\n",
    "\n",
    "- Seminar (10 points): Autoregressive Transformer\n",
    "- Task 1 (10 points): PixelCNN\n",
    "- **Task 2 (10 points): RealNVP**\n",
    "- \\* Bonus (10+++ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842281",
   "metadata": {},
   "source": [
    "## Task 2. RealNVP\n",
    "\n",
    "In this part, implement a simple, but fully operational RealNVP architecture to model flows from binary MNIST and shapes images to $N(0, I)$\n",
    "\n",
    "Original paper: https://arxiv.org/abs/1605.08803\n",
    "\n",
    "We recomment the following network design for affine coupling layer:\n",
    "\n",
    "* 1x1 Conv2d\n",
    "* ConvNeXt/ResNet/MobileNet Block x3+\n",
    "* 1x1 Conv2d\n",
    "\n",
    "And the following hyperparameters:\n",
    "\n",
    "* Batch size 128\n",
    "* Learning rate $10^{-3}$\n",
    "* 20+ epochs\n",
    "* AdamW Optimizer\n",
    "* LayerNorm is recommended, but there is no theoretical restrictions for SimpleNet implementation\n",
    "\n",
    "**Remember:** you must dequantize the data for the flow to have stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525de5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_samples(samples, nrow=10, title='Samples', permute=True, binarize=None):\n",
    "    if permute:\n",
    "        samples = (torch.FloatTensor(samples)).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    img = grid_img.permute(1, 2, 0).numpy()\n",
    "    if binarize is not None:\n",
    "        img = (img > binarize).astype(np.float32)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(fname, include_labels=False):\n",
    "    with open(fname, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "    \n",
    "    if include_labels:\n",
    "        return (data['train'] > 127.5), (data['test'] > 127.5), data['train_labels'], data['test_labels']\n",
    "    \n",
    "    return (data['train'] > 127.5), (data['test'] > 127.5)\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(X) == len(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ceb89",
   "metadata": {},
   "source": [
    "First of all, let's create a function for checkerboard masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbad2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkerboard_mask(size, mask_type=1):\n",
    "    # if type == 1, the top left corner should be 1\n",
    "    # if type == 0, the top left corner should be 0\n",
    "    # function expected to return a 2-d array\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(checkerboard_mask(2, 1).flatten(), [1., 0., \n",
    "                                                       0., 1.], atol=1e-6)\n",
    "assert np.allclose(checkerboard_mask(2, 0).flatten(), [0., 1., \n",
    "                                                       1., 0.], atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d857fc",
   "metadata": {},
   "source": [
    "Here you should build your RealNVP blocks\n",
    "\n",
    "Fill the gaps, comments will help you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, internal=16):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        self.net = # TODO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AffineCheckerboardTransform(nn.Module):\n",
    "    def __init__(self, in_channels: int, size: int, mask_type=1):\n",
    "        super(AffineCheckerboardTransform, self).__init__()\n",
    "        self.mask = torch.tensor(checkerboard_mask(size, mask_type=mask_type).astype('float32')).to(device)\n",
    "        self.scale = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "        self.net = SimpleNet(in_channels, in_channels * 2)\n",
    "        \n",
    "    def forward(self, x, reverse=False):\n",
    "        # returns transform(x), log_det\n",
    "        batch_size, n_channels, _, _ = x.shape\n",
    "        mask = self.mask.repeat(batch_size, 1, 1, 1)\n",
    "        x_ = x * mask\n",
    "\n",
    "        log_s, t = self.net(x_).split(n_channels, dim=1)\n",
    "        log_s = self.scale * torch.tanh(log_s)\n",
    "        # TODO: apply masking to the result\n",
    "\n",
    "        if reverse:  # inverting the transformation\n",
    "            # update x\n",
    "        else:\n",
    "            # update x\n",
    "        return x, log_s\n",
    "\n",
    "\n",
    "class AffineChannelTransform(nn.Module):\n",
    "    def __init__(self, in_channels: int, modify_top: bool):\n",
    "        super(AffineChannelTransform, self).__init__()\n",
    "        self.modify_top = modify_top\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.net = SimpleNet(in_channels=in_channels, out_channels=in_channels * 2)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        n_channels = x.shape[1]\n",
    "        if self.modify_top:\n",
    "            on, off = x.split(n_channels // 2, dim=1)\n",
    "        else:\n",
    "            off, on = x.split(n_channels // 2, dim=1)\n",
    "        log_s, t = self.net(off).split(n_channels // 2, dim=1)\n",
    "        log_s = self.scale * torch.tanh(log_s)\n",
    "\n",
    "        if reverse:  # inverting the transformation\n",
    "            # TODO: update on\n",
    "        else:\n",
    "            # TODO: update on\n",
    "\n",
    "        if self.modify_top:\n",
    "            return # TODO: x, logdet\n",
    "        else:\n",
    "            return # TODO: x, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22917749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Glow, you may use it as is\n",
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(ActNorm, self).__init__()\n",
    "        self.log_scale = nn.Parameter(torch.zeros(1, n_channels, 1, 1), requires_grad=True)\n",
    "        self.shift = nn.Parameter(torch.zeros(1, n_channels, 1, 1), requires_grad=True)\n",
    "        self.n_channels = n_channels\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        if reverse:\n",
    "            return (x - self.shift) * torch.exp(-self.log_scale), self.log_scale\n",
    "        else:\n",
    "            if not self.initialized:\n",
    "                self.shift.data = -torch.mean(x, dim=[0, 2, 3], keepdim=True)\n",
    "                self.log_scale.data = -torch.log(\n",
    "                    torch.std(x.permute(1, 0, 2, 3).reshape(self.n_channels, -1), dim=1).reshape(1, self.n_channels, 1,\n",
    "                                                                                                 1))\n",
    "                self.initialized = True\n",
    "                result = x * torch.exp(self.log_scale) + self.shift\n",
    "            return x * torch.exp(self.log_scale) + self.shift, self.log_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da0503",
   "metadata": {},
   "source": [
    "And RealNVP itself\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31021de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, in_channels, size):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.prior = torch.distributions.Normal(torch.tensor(0.).to(device), torch.tensor(1.).to(device))\n",
    "        self.size = size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_size = size // 4\n",
    "        self.out_channels = in_channels * 16\n",
    "        \n",
    "        self.checker_transforms1 = nn.ModuleList([\n",
    "            ################\n",
    "            # YOUR CODE HERE\n",
    "            ###############\n",
    "        ])\n",
    "\n",
    "        self.channel_transforms = nn.ModuleList([\n",
    "            ################\n",
    "            # YOUR CODE HERE\n",
    "            ###############\n",
    "        ])\n",
    "\n",
    "        self.checker_transforms2 = nn.ModuleList([\n",
    "            ################\n",
    "            # YOUR CODE HERE\n",
    "            ###############\n",
    "        ])\n",
    "\n",
    "    def squeeze(self, x):\n",
    "        # C x H x W -> 4C x H/2 x W/2\n",
    "        [B, C, H, W] = list(x.size())\n",
    "        x = x.reshape(B, C, H // 2, 2, W // 2, 2)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4)\n",
    "        x = x.reshape(B, C * 4, H // 2, W // 2)\n",
    "        return x\n",
    "\n",
    "    def undo_squeeze(self, x):\n",
    "        #  4C x H/2 x W/2  ->  C x H x W\n",
    "        [B, C, H, W] = list(x.size())\n",
    "        x = x.reshape(B, C // 4, 2, 2, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.reshape(B, C // 4, H * 2, W * 2)\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, x: torch.Tensor):\n",
    "        # dequantize and normalize x here\n",
    "        x = x.float()\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        return x\n",
    "\n",
    "    def reverse(self, z):\n",
    "        # z -> x\n",
    "        x = z\n",
    "        for op in reversed(self.checker_transforms2):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        x = self.undo_squeeze(x)\n",
    "        for op in reversed(self.channel_transforms):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        x = self.undo_squeeze(x)\n",
    "        for op in reversed(self.checker_transforms1):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # maps x -> z, and returns the log determinant (not reduced)\n",
    "        # you have reverse implementation, forward can be recovered easily\n",
    "        z, log_det = x, torch.zeros_like(x)\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        return z, log_det\n",
    "\n",
    "    def loss(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        z, log_det = self.forward(x)\n",
    "        logprob = # calculate logprob\n",
    "        return -logprob.mean() / self.size / self.size / self.in_channels\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        with torch.no_grad():\n",
    "            z = self.prior.sample([num_samples, self.out_channels, self.out_size, self.out_size])\n",
    "            x = self.reverse(z)\n",
    "            # TODO: invert your preprocessing (without quantization, of course)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19490ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x, _ in train_loader:\n",
    "        x = x.cuda()\n",
    "        loss = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, _ in data_loader:\n",
    "            x = x.cuda()\n",
    "            loss = model.loss(x)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_loss(model, test_loader)]\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch} started')\n",
    "        model.train()\n",
    "        train_losses.extend(train(model, train_loader, optimizer))\n",
    "        test_loss = eval_loss(model, test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        print('train loss: {}, test_loss: {}'.format(np.mean(train_losses[-1000:]), \n",
    "                                                     test_losses[-1]))\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_model(train_data, test_data, model, dataset_id):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    model: nn.Model item, should contain function loss\n",
    "    dataset_id: in case you want to adjust some settings for each dataset separately \n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - trained model\n",
    "    \"\"\"\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ea581",
   "metadata": {},
   "source": [
    "### First dataset: **Shapes** (5 points)\n",
    "\n",
    "Our reference loss is ~0.23 on test, lower is better. But it could severely different, depening of your dequantization choice\n",
    "\n",
    "Loss should not be negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/shapes.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f62f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_train, shapes_test = load_data('./shapes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e25b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(shapes_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = shapes_train[0].shape\n",
    "model = RealNVP(1, H)\n",
    "train_losses, test_losses, shapes_model = train_model(shapes_train, shapes_test, model, 'shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_plots(train_losses, test_losses, title):\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    plt.ylim(0.4, 2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c184da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = shapes_model.sample(100).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(samples, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold depends on your model and (de)normalization\n",
    "show_samples(samples, permute=False, binarize=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36267e8",
   "metadata": {},
   "source": [
    "### Second dataset: MNIST (5 points)\n",
    "\n",
    "Our reference loss is ~0.17 on test, lower is better. But it could severely different, depening of your dequantization choice\n",
    "\n",
    "Loss should not be negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/mnist.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = load_data('./mnist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6302fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(mnist_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = mnist_train[0].shape\n",
    "model = RealNVP(1, H)\n",
    "train_losses, test_losses, mnist_model = train_model(mnist_train, mnist_test, model, 'mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85579823",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mnist_model.sample(100).cpu()\n",
    "show_samples(samples, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871056c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold depends on your model and (de)normalization\n",
    "show_samples(samples, permute=False, binarize=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
