{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1. Likelihood-based models\n",
    "\n",
    "- Seminar (10 points): Autoregressive Transformer\n",
    "- **Task 1 (10 points): PixelCNN**\n",
    "    - **Unconditional (5 points)**\n",
    "    - Conditional (5 points)\n",
    "- Task 2 (10 points): RealNVP\n",
    "- \\* Bonus (10+++ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 PixelCNN on Shapes and MNIST\n",
    "\n",
    "In this part, implement a simple PixelCNN architecture to model binary MNIST and shapes images\n",
    "\n",
    "Recap:\n",
    "\n",
    "$$Mask_a\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$Mask_b\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We recommend the following network design:\n",
    "* A $7 \\times 7$ masked type A convolution\n",
    "* $5$ $7 \\times 7$ masked type B convolutions\n",
    "* $2$ $1 \\times 1$ masked type B convolutions\n",
    "* Appropriate nonlinearities in-between\n",
    "* 64 convolutional filters\n",
    "* Use normalization carefully: remember about autoregressive property. LayerNorm on channels dimension is definitely OK\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size 128\n",
    "* Learning rate $10^{-3}$\n",
    "* 10 epochs\n",
    "* AdamW Optimizer (this applies to all PixelCNN models trained in future parts)\n",
    "\n",
    "Your model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine). It may also help to scale your input to $[-1, 1]$ before running it through the network. \n",
    "\n",
    "Training on the shapes dataset should be quick, and MNIST should take around 10 minutes\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_samples(samples, nrow=10, title='Samples'):\n",
    "    samples = (torch.FloatTensor(samples)).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(fname, binarize=True, include_labels=False):\n",
    "    with open(fname, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "    \n",
    "    if include_labels:\n",
    "        return (data['train'] > 127.5), (data['test'] > 127.5), data['train_labels'], data['test_labels']\n",
    "    \n",
    "    return (data['train'] > 127.5), (data['test'] > 127.5)\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(X) == len(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to modify `Conv2d` with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, C_in, H_in, W_in) torch.Tensor\n",
    "        Returns\n",
    "          - out (N, C_out, H_out, W_out) should be conv2d(x, weight * mask) + bias \n",
    "        \"\"\"\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    def create_mask(self, mask_type):\n",
    "        assert mask_type == 'A' or mask_type == 'B'\n",
    "        k = self.kernel_size[0]\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convA = MaskedConv2D('A', 1, 1, kernel_size=3)\n",
    "convB = MaskedConv2D('B', 1, 1, kernel_size=3)\n",
    "assert np.allclose(convA.mask.view(-1), [1., 1., 1., 1., 0., 0., 0., 0., 0.], atol=1e-6)\n",
    "assert np.allclose(convB.mask.view(-1), [1., 1., 1., 1., 1., 0., 0., 0., 0.], atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_colors=2, n_filters=64,\n",
    "               kernel_size=7, n_layers=5):\n",
    "        super().__init__()\n",
    "        assert n_layers >= 2\n",
    "        n_channels = input_shape[0]\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.n_channels = n_channels\n",
    "        self.n_colors = n_colors\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        \n",
    "    def forward(self, x, cond=None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = (x.float() / (self.n_colors - 1) - 0.5) / 0.5\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "    def loss(self, x):\n",
    "        return F.cross_entropy(self.forward(x), x.long())\n",
    "\n",
    "    def sample(self, n):\n",
    "        samples = torch.zeros(n, *self.input_shape).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[1]):\n",
    "                for c in range(self.input_shape[2]):\n",
    "                    for k in range(self.n_channels):\n",
    "                        logits = self.forward(samples)[:, :, k, r, c]\n",
    "                        probs = F.softmax(logits, dim=1)\n",
    "                        samples[:, k, r, c] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        return samples.permute(0, 2, 3, 1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use and modify this train loop. You may want to show some logs or sampling results during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x, _ in train_loader:\n",
    "        x = x.cuda()\n",
    "        loss = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, _ in data_loader:\n",
    "            x = x.cuda()\n",
    "            loss = model.loss(x)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_loss(model, test_loader)]\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch} started')\n",
    "        model.train()\n",
    "        train_losses.extend(train(model, train_loader, optimizer))\n",
    "        test_loss = eval_loss(model, test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        print('train loss: {}, test_loss: {}'.format(np.mean(train_losses[-1000:]), \n",
    "                                                     test_losses[-1]))\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_model(train_data, test_data, model):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    model: nn.Model item, should contain function loss and accept\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - trained model\n",
    "    \"\"\"\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First dataset: **Shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/shapes.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_train, shapes_test = load_data('./shapes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(shapes_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = shapes_train[0].shape\n",
    "model = PixelCNN((1, H, W))\n",
    "train_losses, test_losses, shapes_model = train_model(shapes_train, shapes_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_plots(train_losses, test_losses, title):\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = shapes_model.sample(100)\n",
    "show_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second dataset: **MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "# ! wget https://github.com/a4-edu/course_gmcv/raw/hw1/module1-likelihood/shapes.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = load_data('./mnist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(mnist_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = mnist_train[0].shape\n",
    "model = PixelCNN((1, H, W))\n",
    "train_losses, test_losses, mnist_model = train_model(mnist_train, mnist_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, test_losses, 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mnist_model.sample(100)\n",
    "show_samples(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
