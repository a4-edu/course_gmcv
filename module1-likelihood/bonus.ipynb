{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1. Likelihood-based models\n",
    "\n",
    "- Seminar (10 points): Autoregressive Transformer\n",
    "- Task 1 (10 points): PixelCNN\n",
    "- Task 2 (10 points): RealNVP\n",
    "- **\\* Bonus (10+++ points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "There are some additional tasks you can handle around PixelCNN models\n",
    "\n",
    "### PixelCNN with RGB support\n",
    "\n",
    "Use `shapes_colored.pkl` and `mnist_colored.pkl` from `colored_data.zip` for your generative model\n",
    "\n",
    "- (5 points) Implement a PixelCNN that assumes color channels as **independent**. More formally, we model the following parameterized distribution: $$p_\\theta(x) = \\prod_{i=1}^{HW}\\prod_{c=1}^C p_\\theta(x_i^c | x_{<i})$$ \n",
    "- (5 points) Implement a PixelCNN that models **dependent** color channels. Formally, we model the parameterized distribution $$p_\\theta(x) = \\prod_{i=1}^{HW}\\prod_{c=1}^C p_\\theta(x_i^c | x_i^{<c}, x_{<i})$$\n",
    "- (10++ points) If you feel the power overwhelming, try to train PixelCNN on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "Here are some tips that you may find useful for designing and training these models:\n",
    "* You will need a 4-way softmax for every prediction, as opposed to a 256-way softmax in the PixelCNN paper, since the dataset is quantized to two bits per color channel\n",
    "* You can set number of filters for each convolutions to 120. You can use the ReLU nonlinearity throughout.\n",
    "* Use a stack of 8 residual block architecture from [Figure 5](https://arxiv.org/abs/1601.06759) but with 7 x 7 masked convolutions in the middle instead of 3 x 3 masked convolutions\n",
    "* Consider using [layer normalization](https://arxiv.org/abs/1607.06450) to improve performance. However, be careful to maintain the autoregressive property. Do not try to use batch normalization, it's not working with autoregressive models at all\n",
    "* With a learning rate of $10^{-3}$ and a batch size of 128, it should take a few minutes to run on the shapes dataset, and about 50-60 minutes on MNIST.\n",
    "\n",
    "Report training curves, final test loss, and samples\n",
    "\n",
    "### PixelCNN variations\n",
    "\n",
    "- (10 points) Implement a [Gated PixelCNN](https://arxiv.org/abs/1606.05328) to fix the blind-spot issue. Train it on binary MNIST, and report training curves, final test loss, and samples.\n",
    "- (10 points) Train a [Grayscale PixelCNN](https://arxiv.org/abs/1612.08185) on Colored MNIST. You do not need to use their architecture - stacking standard masked convolutions or residual blocks is fine. First, generate a binary image, and then the 2-bit color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
