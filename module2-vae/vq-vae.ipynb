{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEcSNKhrotPo"
   },
   "source": [
    "# Homework 2. Latent Variable Models\n",
    "\n",
    "- VAEs on 2D Data (5 points)\n",
    "- VAEs on images (5 points)\n",
    "- **VQ-VAE on images (20 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7P6yaWwFXOQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "822YbCb2fsz8"
   },
   "source": [
    "## VQ-VAE (20 points) \n",
    "\n",
    "You will train a [VQ-VAE](https://arxiv.org/abs/1711.00937) on CIFAR-10 and SVHN. If you are confused on how the VQ-VAE works, you may find [Lilian Weng's blogpost](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vq-vae-and-vq-vae-2) to be useful.\n",
    "\n",
    "You may experiment with different hyperparameters and architecture designs, but the following designs for the VQ-VAE architecture may be useful.\n",
    "\n",
    "```\n",
    "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "linear(in_dim, out_dim)\n",
    "batch_norm2d(dim)\n",
    "\n",
    "residual_block(dim)\n",
    "    batch_norm2d(dim)\n",
    "    relu()\n",
    "    conv2d(dim, dim, 3, 1, 1)\n",
    "    batch_norm2d(dim)\n",
    "    relu()\n",
    "    conv2d(dim, dim, 1, 1, 0)\n",
    "\n",
    "Encoder\n",
    "    conv2d(3, 256, 4, 2, 1) 16 x 16\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    conv2d(256, 256, 4, 2, 1) 8 x 8\n",
    "    residual_block(256)\n",
    "    residual_block(256)\n",
    "\n",
    "Decoder\n",
    "    residual_block(256)\n",
    "    residual_block(256)\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    transpose_conv2d(256, 256, 4, 2, 1) 16 x 16\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    transpose_conv2d(256, 3, 4, 2, 1) 32 x 32\n",
    "```\n",
    "\n",
    "A few other tips:\n",
    "*   Use a codebook with $K = 128$ latents each with a $D = 256$ dimensional embedding vector\n",
    "*   You should initialize each element in your $K\\times D$ codebook to be uniformly random in $[-1/K, 1/K]$\n",
    "*   Use batch size 128 with a learning rate of $10^{-3}$ and an Adam optimizer\n",
    "*   Center and scale your images to $[-1, 1]$\n",
    "*   Supposing that $z_e(x)$ is the encoder output, and $z_q(x)$ is the quantized output using the codebook, you can implement the straight-through estimator as follows (where below is fed into the decoder): \n",
    "    * `(z_q(x) - z_e(x)).detach() + z_e(x)` in Pytorch\n",
    "\n",
    "In addition to training the VQ-VAE, you will also need to train a PixelCNN prior on the categorical latents in order to sample. For your architecture, you may find the following useful:\n",
    "*   Since the input is a 2D grid of discrete values, you should have an input (learned) embedding layer to map the discrete values to embeddings of length $64$\n",
    "*   Use a single Type A masked convolutions followed by 10-15 residual blocks, and $2$ $1\\times 1$ convolutions of $512$ and $K$ channels respectively.\n",
    "*   You may find normalization methods (e.g. LayerNorm) to be useful. But do not forget about autoregressive property in PixelCNN, use normalization carefully!\n",
    "*   Use batch size 128 with a learning rate of $10^{-3}$ and an Adam optimizer\n",
    "\n",
    "Alternatively, you could train Transformer prior model instead of PixelCNN\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average loss of the training data (per minibatch) and test data (for your entire test set) **for both your VQ-VAE and PixelCNN/Transformer prior**\n",
    "2. Report the final test set performances of your final models\n",
    "3. 100 samples from your trained VQ-VAE and PixelCNN prior\n",
    "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import SVHN, CIFAR10\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "def show_samples(samples, nrow=10, title='Samples'):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "DATA_DIR = './data'\n",
    "def get_cifar10():\n",
    "    train = CIFAR10(root=f'{DATA_DIR}/cifar10', train=True, download=True).data\n",
    "    test = CIFAR10(root=f'{DATA_DIR}/cifar10', train=False).data\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_svhn():\n",
    "    train = SVHN(root=f'{DATA_DIR}/svhn', split='train', download=True).data.transpose(0, 2, 3, 1)\n",
    "    test = SVHN(root=f'{DATA_DIR}/svhn', split='test', download=True).data.transpose(0, 2, 3, 1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def visualize_cifar10():\n",
    "    _, test = get_cifar10()\n",
    "    samples = test[np.random.choice(len(test), 100)]\n",
    "    show_samples(samples, title=\"CIFAR10 samples\")\n",
    "\n",
    "\n",
    "def visualize_svhn():\n",
    "    _, test = get_svhn()\n",
    "    print(test.shape)\n",
    "    samples = test[np.random.choice(len(test), 100)]\n",
    "    show_samples(samples, title=\"SVHN samples\")\n",
    "\n",
    "visualize_cifar10()\n",
    "visualize_svhn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwT1tOdm0e84"
   },
   "source": [
    "### Solution\n",
    "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpqhOqW1UDby"
   },
   "outputs": [],
   "source": [
    "def q3(train_data, test_data, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 32, 32, 3) uint8 numpy array of color images with values in [0, 255]\n",
    "    test_data: An (n_test, 32, 32, 3) uint8 numpy array of color images with values in [0, 255]\n",
    "    dset_id: An identifying number of which dataset is given ('cifar' or 'svhn'). Most likely\n",
    "               used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of VQ-VAE train losess evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of VQ-VAE train losses evaluated once at initialization and after each epoch\n",
    "    - a (# of training iterations,) numpy array of PixelCNN prior train losess evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of PixelCNN prior train losses evaluated once at initialization and after each epoch\n",
    "    - a (100, 32, 32, 3) numpy array of 100 samples with values in {0, ... 255}\n",
    "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
    "      FROM THE TEST SET with values in [0, 255]\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qvP94pm0hYb"
   },
   "source": [
    "### Results\n",
    "Once you've finished `q3`, execute the cells below to visualize and save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPtDMRpf0iAG"
   },
   "outputs": [],
   "source": [
    "def show_training_plot(train_losses, test_losses, title):\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def q3_results(dset_id, fn):\n",
    "    if dset_id.lower() == 'cifar':\n",
    "        train_data, test_data = get_cifar10()\n",
    "    elif dset_id.lower() == 'svhn':\n",
    "        train_data, test_data = get_svhn()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "\n",
    "    vqvae_train_losses, vqvae_test_losses, pixelcnn_train_losses, pixelcnn_test_losses, \\\n",
    "        samples, reconstructions = fn(train_data, test_data, dset_id)\n",
    "    samples, reconstructions = samples.astype('float32'), reconstructions.astype('float32')\n",
    "    print(f'VQ-VAE Final Test Loss: {vqvae_test_losses[-1]:.4f}')\n",
    "    print(f'PixelCNN Prior Final Test Loss: {pixelcnn_test_losses[-1]:.4f}')\n",
    "    show_training_plot(vqvae_train_losses, vqvae_test_losses, \n",
    "                           f'(VQ-VAE) Dataset {dset_id} Train Plot')\n",
    "    show_training_plot(pixelcnn_train_losses, pixelcnn_test_losses, \n",
    "                           f'(PixelCNN) Dataset {dset_id} Train Plot')\n",
    "    show_samples(samples, title=f'{dset_id} samples')\n",
    "    show_samples(reconstructions, title=f'{dset_id} Reconstructions')\n",
    "\n",
    "\n",
    "q3_results('cifar', q3)\n",
    "q3_results('svhn', q3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "module2_latent_variable_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
